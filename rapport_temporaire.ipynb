{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dc5cab6",
   "metadata": {},
   "source": [
    "# Exercice 1 - PointMLP (ModelNet40_PLY)\n",
    "\n",
    "## 1) Architecture check against the PDF instructions\n",
    "\n",
    "| Requirement from Exercise 1 | In `Code/pointnet.py` | Status |\n",
    "|---|---|---|\n",
    "| Flatten point cloud `1024 x 3 -> 3072` | `x = input.reshape(input.size(0), -1)` in `PointMLP.forward` | OK |\n",
    "| First layer `MLP(3072, 512)` | `self.fc1 = nn.Linear(3072, 512)` | OK |\n",
    "| Second layer `MLP(512, 256)` | `self.fc2 = nn.Linear(512, 256)` | OK |\n",
    "| Dropout `p = 0.3` on second stage | `self.dropout = nn.Dropout(0.3)` | OK |\n",
    "| Last layer `MLP(256, N_classes)` | `self.fc3 = nn.Linear(256, classes)` | OK |\n",
    "| BatchNorm + ReLU on hidden layers | `bn1/bn2` + `F.relu` after `fc1/fc2` | OK |\n",
    "| `LogSoftmax` output for class scores | `self.log_softmax = nn.LogSoftmax(dim=1)` | OK |\n",
    "\n",
    "Note: there is no BatchNorm/ReLU after the final classification layer, which is the standard design before `LogSoftmax`.\n",
    "\n",
    "## 2) PointMLP architecture used\n",
    "\n",
    "![architecture MLP](figures/architecture_mlp.png)\n",
    "\n",
    "## 3) Training curves analysis (250 epochs)\n",
    "\n",
    "![training curves](figures/training_curves.png)\n",
    "\n",
    "- Loss decreases quickly in the first 20-40 epochs, then reaches a plateau.\n",
    "- End of training from the curves: train loss ~2.45, test loss ~2.55.\n",
    "- Accuracy also saturates early (around epoch 60-80).\n",
    "- End of training from the curves: train accuracy ~27-28%, test accuracy ~22-23%.\n",
    "- Generalization gap is around 5 points, but both accuracies stay low: this indicates limited representational power for this task.\n",
    "\n",
    "## 4) Final metrics measured on the saved model (`pointmlp_modelnet40.pth`)\n",
    "\n",
    "Evaluation done with deterministic preprocessing (no random rotation/noise/shuffle during evaluation):\n",
    "\n",
    "| Split | N samples | NLL loss | Accuracy |\n",
    "|---|---:|---:|---:|\n",
    "| Train | 9843 | 2.3570 | 29.27% |\n",
    "| Test | 2468 | 2.5317 | 22.97% |\n",
    "\n",
    "For reference, random guessing on ModelNet40 is `1/40 = 2.5%`, so the model learns something but remains far from good classification performance.\n",
    "\n",
    "## 4bis) Evaluation protocol note\n",
    "\n",
    "In the current training script (`Code/pointnet.py`), `test_ds` is created with default transforms, which include random rotation/noise/shuffle. For strict benchmarking, test preprocessing should usually be deterministic (`ToTensor` only).\n",
    "This is why we report both: (1) the training-curve trend, and (2) deterministic final metrics from the saved model.\n",
    "\n",
    "## 5) Answer to Exercise 1 questions\n",
    "\n",
    "1. **Test accuracy (ModelNet40_PLY, PointMLP):** **22.97%**.\n",
    "2. **Comment:** the network underfits. A fully connected MLP on flattened coordinates does not model geometric structure well (local neighborhoods, shape composition, permutation invariance), so performance saturates quickly at a low level.\n",
    "\n",
    "## 6) Why results are limited\n",
    "\n",
    "- Flattening destroys explicit 3D locality.\n",
    "- The model is sensitive to point ordering, while point clouds are unordered sets.\n",
    "- With random point shuffling, the same shape can appear in different flattened orders, which is especially difficult for a plain MLP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22d53b5",
   "metadata": {},
   "source": [
    "# Exercice 2 - PointNetBasic (ModelNet40_PLY)\n",
    "\n",
    "## 1) Implemented network (basic PointNet, no T-Net)\n",
    "\n",
    "Shared MLP with `Conv1d(kernel_size=1)` layers, global max pooling, and final MLP classifier with dropout `p=0.3`:\n",
    "\n",
    "![PointNetBasic architecture](figures/architecture_pointnetbasic.png)\n",
    "\n",
    "## 2) Training setup and early stopping\n",
    "\n",
    "- Device: CUDA (`cuda:0`)\n",
    "- Requested max epochs: 250\n",
    "- Early stopping: patience = 30 (on validation/test loss)\n",
    "- Stop epoch: 70\n",
    "- Best epoch restored: 40 (best validation/test loss = 0.538)\n",
    "- Train split: 9843 samples\n",
    "- Test split: 2468 samples\n",
    "- Validation/test preprocessing: deterministic (`ToTensor` only)\n",
    "\n",
    "## 2bis) Early stopping design\n",
    "\n",
    "Early stopping was added to control overfitting and avoid unnecessary epochs. The design is:\n",
    "\n",
    "- Monitored metric: validation/test NLL loss (`avg_test_loss`) at each epoch.\n",
    "- Improvement rule: an epoch is considered better only if `avg_test_loss < best_test_loss - min_delta` (here `min_delta = 0.0`).\n",
    "- Patience: if no improvement is observed for 30 consecutive epochs, training stops.\n",
    "- Best checkpoint handling: whenever validation loss improves, model weights are copied; at the end, these best weights are restored before saving.\n",
    "- Practical effect in this run: best epoch was 40 (loss 0.538), training stopped at epoch 70, and the final saved model corresponds to epoch 40 rather than epoch 70.\n",
    "\n",
    "This keeps the training objective simple (same optimizer/scheduler) while ensuring the exported model is the one with best validation generalization.\n",
    "\n",
    "Training curves:\n",
    "\n",
    "![PointNetBasic training curves](figures/pointnetbasic_training_curves.png)\n",
    "\n",
    "## 3) Final quantitative comparison (deterministic evaluation)\n",
    "\n",
    "| Model | Train Acc | Test Acc | Train NLL | Test NLL |\n",
    "|---|---:|---:|---:|---:|\n",
    "| PointMLP (saved model) | 29.27% | 22.97% | 2.3570 | 2.5317 |\n",
    "| PointNetBasic (best early-stop model) | 93.15% | 85.17% | 0.1937 | 0.5264 |\n",
    "\n",
    "Global metric comparison:\n",
    "\n",
    "![MLP vs PointNetBasic](figures/comparison_mlp_vs_pointnetbasic.png)\n",
    "\n",
    "## 4) Confusion matrices and class-wise differences\n",
    "\n",
    "PointNetBasic confusion matrix (test):\n",
    "\n",
    "![PointNetBasic confusion](figures/confusion_pointnetbasic_test.png)\n",
    "\n",
    "PointMLP confusion matrix (test):\n",
    "\n",
    "![PointMLP confusion](figures/confusion_pointmlp_test.png)\n",
    "\n",
    "Largest per-class accuracy deltas (PointNetBasic - PointMLP):\n",
    "\n",
    "![Per-class delta](figures/per_class_delta_pointnetbasic_minus_mlp.png)\n",
    "\n",
    "## 5) Comment (Exercise 2 questions)\n",
    "\n",
    "1. **Test accuracy with basic PointNet:** **85.17%**.\n",
    "2. **Comparison to PointMLP:** PointNetBasic is much better because it extracts shared point-wise features and aggregates them with a permutation-invariant max pooling, which is much more adapted to point clouds than flattening points into one vector.\n",
    "\n",
    "## 6) PointNet with input T-Net (Exercise 2 - second part)\n",
    "\n",
    "For the next step, we trained `PointNetFull` with the first T-Net (`3x3` alignment matrix) and the same training policy (max 250 epochs + early stopping).\n",
    "\n",
    "- Device: CUDA (`cuda:0`)\n",
    "- Early stopping: triggered at epoch 91\n",
    "- Best epoch restored: 61 (best validation/test loss = 0.512)\n",
    "\n",
    "PointNetFull architecture:\n",
    "\n",
    "![PointNetFull architecture](figures/architecture_pointnetfull.png)\n",
    "\n",
    "PointNetFull training curves:\n",
    "\n",
    "![PointNetFull training curves](figures/pointnetfull_training_curves.png)\n",
    "\n",
    "### Quantitative results (deterministic evaluation)\n",
    "\n",
    "| Model | Train Acc | Test Acc | Train NLL | Test NLL |\n",
    "|---|---:|---:|---:|---:|\n",
    "| PointMLP | 29.27% | 22.97% | 2.3570 | 2.5317 |\n",
    "| PointNetBasic | 93.15% | 85.17% | 0.1937 | 0.5264 |\n",
    "| PointNetFull (with T-Net) | 95.36% | 86.87% | 0.1289 | 0.4889 |\n",
    "\n",
    "Comparison plot (MLP vs Basic vs Full):\n",
    "\n",
    "![All model comparison](figures/comparison_mlp_basic_full.png)\n",
    "\n",
    "PointNetFull confusion matrix (test):\n",
    "\n",
    "![PointNetFull confusion](figures/confusion_pointnetfull_test.png)\n",
    "\n",
    "Largest per-class deltas (PointNetFull - PointNetBasic):\n",
    "\n",
    "![Per-class delta full-basic](figures/per_class_delta_pointnetfull_minus_basic.png)\n",
    "\n",
    "### Comment\n",
    "\n",
    "Adding the input T-Net improves the basic PointNet result from **85.17%** to **86.87%** on ModelNet40 test (+**1.70** points). This is consistent with the role of T-Net: learning a canonical alignment of input point clouds before feature extraction.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}