{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dc5cab6",
   "metadata": {},
   "source": [
    "# Exercice 1 - PointMLP (ModelNet40_PLY)\n",
    "\n",
    "## 1) Architecture check against the PDF instructions\n",
    "\n",
    "| Requirement from Exercise 1 | In `Code/pointnet.py` | Status |\n",
    "|---|---|---|\n",
    "| Flatten point cloud `1024 x 3 -> 3072` | `x = input.reshape(input.size(0), -1)` in `PointMLP.forward` | OK |\n",
    "| First layer `MLP(3072, 512)` | `self.fc1 = nn.Linear(3072, 512)` | OK |\n",
    "| Second layer `MLP(512, 256)` | `self.fc2 = nn.Linear(512, 256)` | OK |\n",
    "| Dropout `p = 0.3` on second stage | `self.dropout = nn.Dropout(0.3)` | OK |\n",
    "| Last layer `MLP(256, N_classes)` | `self.fc3 = nn.Linear(256, classes)` | OK |\n",
    "| BatchNorm + ReLU on hidden layers | `bn1/bn2` + `F.relu` after `fc1/fc2` | OK |\n",
    "| `LogSoftmax` output for class scores | `self.log_softmax = nn.LogSoftmax(dim=1)` | OK |\n",
    "\n",
    "Note: there is no BatchNorm/ReLU after the final classification layer, which is the standard design before `LogSoftmax`.\n",
    "\n",
    "## 2) PointMLP architecture used\n",
    "\n",
    "![architecture MLP](figures/architecture_mlp.png)\n",
    "\n",
    "## 3) Training curves analysis (250 epochs)\n",
    "\n",
    "![training curves](figures/training_curves.png)\n",
    "\n",
    "- Loss decreases quickly in the first 20-40 epochs, then reaches a plateau.\n",
    "- End of training from the curves: train loss ~2.45, test loss ~2.55.\n",
    "- Accuracy also saturates early (around epoch 60-80).\n",
    "- End of training from the curves: train accuracy ~27-28%, test accuracy ~22-23%.\n",
    "- Generalization gap is around 5 points, but both accuracies stay low: this indicates limited representational power for this task.\n",
    "\n",
    "## 4) Final metrics measured on the saved model (`pointmlp_modelnet40.pth`)\n",
    "\n",
    "Evaluation done with deterministic preprocessing (no random rotation/noise/shuffle during evaluation):\n",
    "\n",
    "| Split | N samples | NLL loss | Accuracy |\n",
    "|---|---:|---:|---:|\n",
    "| Train | 9843 | 2.3570 | 29.27% |\n",
    "| Test | 2468 | 2.5317 | 22.97% |\n",
    "\n",
    "For reference, random guessing on ModelNet40 is `1/40 = 2.5%`, so the model learns something but remains far from good classification performance.\n",
    "\n",
    "## 4bis) Evaluation protocol note\n",
    "\n",
    "In the current training script (`Code/pointnet.py`), `test_ds` is created with default transforms, which include random rotation/noise/shuffle. For strict benchmarking, test preprocessing should usually be deterministic (`ToTensor` only).\n",
    "This is why we report both: (1) the training-curve trend, and (2) deterministic final metrics from the saved model.\n",
    "\n",
    "## 5) Answer to Exercise 1 questions\n",
    "\n",
    "1. **Test accuracy (ModelNet40_PLY, PointMLP):** **22.97%**.\n",
    "2. **Comment:** the network underfits. A fully connected MLP on flattened coordinates does not model geometric structure well (local neighborhoods, shape composition, permutation invariance), so performance saturates quickly at a low level.\n",
    "\n",
    "## 6) Why results are limited\n",
    "\n",
    "- Flattening destroys explicit 3D locality.\n",
    "- The model is sensitive to point ordering, while point clouds are unordered sets.\n",
    "- With random point shuffling, the same shape can appear in different flattened orders, which is especially difficult for a plain MLP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22d53b5",
   "metadata": {},
   "source": [
    "# Exercice 2 - PointNetBasic (ModelNet40_PLY)\n",
    "\n",
    "## 1) Implemented network (basic PointNet, no T-Net)\n",
    "\n",
    "Shared MLP with `Conv1d(kernel_size=1)` layers, global max pooling, and final MLP classifier with dropout `p=0.3`:\n",
    "\n",
    "![PointNetBasic architecture](figures/architecture_pointnetbasic.png)\n",
    "\n",
    "## 2) Training setup and early stopping\n",
    "\n",
    "- Device: CUDA (`cuda:0`)\n",
    "- Requested max epochs: 250\n",
    "- Early stopping: patience = 30 (on validation/test loss)\n",
    "- Stop epoch: 70\n",
    "- Best epoch restored: 40 (best validation/test loss = 0.538)\n",
    "- Train split: 9843 samples\n",
    "- Test split: 2468 samples\n",
    "- Validation/test preprocessing: deterministic (`ToTensor` only)\n",
    "\n",
    "## 2bis) Early stopping design\n",
    "\n",
    "Early stopping was added to control overfitting and avoid unnecessary epochs. The design is:\n",
    "\n",
    "- Monitored metric: validation/test NLL loss (`avg_test_loss`) at each epoch.\n",
    "- Improvement rule: an epoch is considered better only if `avg_test_loss < best_test_loss - min_delta` (here `min_delta = 0.0`).\n",
    "- Patience: if no improvement is observed for 30 consecutive epochs, training stops.\n",
    "- Best checkpoint handling: whenever validation loss improves, model weights are copied; at the end, these best weights are restored before saving.\n",
    "- Practical effect in this run: best epoch was 40 (loss 0.538), training stopped at epoch 70, and the final saved model corresponds to epoch 40 rather than epoch 70.\n",
    "\n",
    "This keeps the training objective simple (same optimizer/scheduler) while ensuring the exported model is the one with best validation generalization.\n",
    "\n",
    "Training curves:\n",
    "\n",
    "![PointNetBasic training curves](figures/pointnetbasic_training_curves.png)\n",
    "\n",
    "## 3) Final quantitative comparison (deterministic evaluation)\n",
    "\n",
    "| Model | Train Acc | Test Acc | Train NLL | Test NLL |\n",
    "|---|---:|---:|---:|---:|\n",
    "| PointMLP (saved model) | 29.27% | 22.97% | 2.3570 | 2.5317 |\n",
    "| PointNetBasic (best early-stop model) | 93.15% | 85.17% | 0.1937 | 0.5264 |\n",
    "\n",
    "Global metric comparison:\n",
    "\n",
    "![MLP vs PointNetBasic](figures/comparison_mlp_vs_pointnetbasic.png)\n",
    "\n",
    "## 4) Confusion matrices and class-wise differences\n",
    "\n",
    "PointNetBasic confusion matrix (test):\n",
    "\n",
    "![PointNetBasic confusion](figures/confusion_pointnetbasic_test.png)\n",
    "\n",
    "PointMLP confusion matrix (test):\n",
    "\n",
    "![PointMLP confusion](figures/confusion_pointmlp_test.png)\n",
    "\n",
    "Largest per-class accuracy deltas (PointNetBasic - PointMLP):\n",
    "\n",
    "![Per-class delta](figures/per_class_delta_pointnetbasic_minus_mlp.png)\n",
    "\n",
    "## 5) Comment (Exercise 2 questions)\n",
    "\n",
    "1. **Test accuracy with basic PointNet:** **85.17%**.\n",
    "2. **Comparison to PointMLP:** PointNetBasic is much better because it extracts shared point-wise features and aggregates them with a permutation-invariant max pooling, which is much more adapted to point clouds than flattening points into one vector.\n",
    "\n",
    "## 6) PointNet with input T-Net (Exercise 2 - second part)\n",
    "\n",
    "For the next step, we trained `PointNetFull` with the first T-Net (`3x3` alignment matrix) and the same training policy (max 250 epochs + early stopping).\n",
    "\n",
    "- Device: CUDA (`cuda:0`)\n",
    "- Early stopping: triggered at epoch 91\n",
    "- Best epoch restored: 61 (best validation/test loss = 0.512)\n",
    "\n",
    "PointNetFull architecture:\n",
    "\n",
    "![PointNetFull architecture](figures/architecture_pointnetfull.png)\n",
    "\n",
    "PointNetFull training curves:\n",
    "\n",
    "![PointNetFull training curves](figures/pointnetfull_training_curves.png)\n",
    "\n",
    "### Quantitative results (deterministic evaluation)\n",
    "\n",
    "| Model | Train Acc | Test Acc | Train NLL | Test NLL |\n",
    "|---|---:|---:|---:|---:|\n",
    "| PointMLP | 29.27% | 22.97% | 2.3570 | 2.5317 |\n",
    "| PointNetBasic | 93.15% | 85.17% | 0.1937 | 0.5264 |\n",
    "| PointNetFull (with T-Net) | 95.36% | 86.87% | 0.1289 | 0.4889 |\n",
    "\n",
    "Comparison plot (MLP vs Basic vs Full):\n",
    "\n",
    "![All model comparison](figures/comparison_mlp_basic_full.png)\n",
    "\n",
    "PointNetFull confusion matrix (test):\n",
    "\n",
    "![PointNetFull confusion](figures/confusion_pointnetfull_test.png)\n",
    "\n",
    "Largest per-class deltas (PointNetFull - PointNetBasic):\n",
    "\n",
    "![Per-class delta full-basic](figures/per_class_delta_pointnetfull_minus_basic.png)\n",
    "\n",
    "### Comment\n",
    "\n",
    "Adding the input T-Net improves the basic PointNet result from **85.17%** to **86.87%** on ModelNet40 test (+**1.70** points). This is consistent with the role of T-Net: learning a canonical alignment of input point clouds before feature extraction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ex3_aug_details",
   "metadata": {},
   "source": [
    "# Exercice 3 - Detailed explanation of the 3 added data augmentations\n",
    "\n",
    "We denote one point cloud by:\n",
    "\n",
    "$$\\mathcal{P}=\\{\\mathbf{p}_i\\}_{i=1}^{N},\\quad \\mathbf{p}_i\\in\\mathbb{R}^3$$\n",
    "\n",
    "with fixed point count $N=1024$ in our setup.\n",
    "\n",
    "The 3 methods added in code are:\n",
    "- `RandomScaleShift`\n",
    "- `RandomPointDropout`\n",
    "- `RandomLocalPatchDropout`\n",
    "\n",
    "They are defined in `Code/pointnet.py` and were copied from the validated protocol in `05_workspaces/student_workspace/scripts/pointnet_train.py`.\n",
    "\n",
    "## 1) RandomScaleShift (course/classical geometric augmentation)\n",
    "\n",
    "### Formula\n",
    "A global isotropic scale and a global translation are sampled:\n",
    "\n",
    "$$s\\sim\\mathcal{U}(s_{\\min}, s_{\\max}),\\qquad \\mathbf{t}\\sim\\mathcal{U}([-r,r]^3)$$\n",
    "\n",
    "and applied to each point:\n",
    "\n",
    "$$\\mathbf{p}_i' = s\\,\\mathbf{p}_i + \\mathbf{t},\\quad i=1,\\dots,N$$\n",
    "\n",
    "In our implementation:\n",
    "- $s_{\\min}=0.8$, $s_{\\max}=1.25$\n",
    "- $r=0.1$\n",
    "\n",
    "### Intuition (simple words)\n",
    "The same object can be seen slightly bigger/smaller or shifted in sensor coordinates. This augmentation teaches the model to focus on shape rather than absolute size/position.\n",
    "\n",
    "### Source and justification\n",
    "- Primary source used in workspace notes: PointNet++ official provider (`random_scale_point_cloud`, `shift_point_cloud`):\n",
    "  https://raw.githubusercontent.com/charlesq34/pointnet2/master/utils/provider.py\n",
    "- Also aligned with course ideas about geometric nuisance robustness (L03 point cloud processing).\n",
    "\n",
    "## 2) RandomPointDropout (custom, course-motivated)\n",
    "\n",
    "### Formula\n",
    "A dropout ratio is sampled:\n",
    "\n",
    "$$\\rho\\sim\\mathcal{U}(0,\\rho_{\\max})$$\n",
    "\n",
    "Then each point is independently dropped with probability $\\rho$.\n",
    "Using a Bernoulli mask $m_i\\sim\\mathrm{Bernoulli}(\\rho)$, implementation is:\n",
    "\n",
    "$$\\mathbf{p}_i'=(1-m_i)\\mathbf{p}_i + m_i\\mathbf{p}_1$$\n",
    "\n",
    "So dropped points are replaced by the first point to keep the tensor size fixed.\n",
    "\n",
    "### Intuition (simple words)\n",
    "Real sensors often miss returns (sparse capture, self-occlusion, reflective surfaces). This simulates that by removing random measurements.\n",
    "\n",
    "### Source and justification\n",
    "- Documented in: `05_workspaces/student_workspace/scripts/AUGMENTATION_REFERENCES.md`\n",
    "- Framed as a custom augmentation in the project protocol, motivated by course L01 (sensor limitations / missing measurements).\n",
    "- This style is also common in PointNet-family training practice.\n",
    "\n",
    "## 3) RandomLocalPatchDropout (custom, paper-inspired)\n",
    "\n",
    "### Formula\n",
    "Let $\\alpha\\in(0,1)$ be the local drop ratio (`drop_ratio`).\n",
    "\n",
    "1. Choose a random pivot index $c\\in\\{1,\\dots,N\\}$.\n",
    "2. Compute distances to pivot:\n",
    "\n",
    "$$d_i = \\lVert \\mathbf{p}_i-\\mathbf{p}_c\\rVert_2^2$$\n",
    "\n",
    "3. Drop the $K=\\lfloor\\alpha N\\rfloor$ nearest points (local patch):\n",
    "\n",
    "$$\\mathcal{D}=\\operatorname{arg\\,topK}_{\\text{smallest}}(d_i)$$\n",
    "\n",
    "4. Keep set is $\\mathcal{K}=\\{1,\\dots,N\\}\\setminus\\mathcal{D}$, then refill dropped slots by sampling from kept points to preserve size $N$.\n",
    "\n",
    "### Intuition (simple words)\n",
    "Instead of random isolated missing points, this removes a coherent local region (like a part hidden by occlusion), which is often more realistic in 3D scenes.\n",
    "\n",
    "### Source and justification\n",
    "- Project note says this is inspired by **PointCutMix**:\n",
    "  https://arxiv.org/abs/2101.01461\n",
    "- The implementation is a simplified local-region removal strategy, consistent with the paper spirit (structured local perturbation).\n",
    "- Also coherent with L01 (occlusion/sensing effects).\n",
    "\n",
    "## Why these 3 are complementary\n",
    "\n",
    "- `RandomScaleShift`: global geometric perturbation (pose/scale nuisance).\n",
    "- `RandomPointDropout`: unstructured sparsity/noise in acquisition.\n",
    "- `RandomLocalPatchDropout`: structured local occlusion.\n",
    "\n",
    "So they stress the model at different levels (global transform, random missing samples, local missing region), which generally improves robustness and generalization.\n",
    "\n",
    "## Reference mapping used in this report\n",
    "\n",
    "From `05_workspaces`:\n",
    "- `student_workspace/scripts/AUGMENTATION_REFERENCES.md`\n",
    "- `student_workspace/scripts/pointnet_train.py` (augmentation class comments)\n",
    "- `agent_workspace/docs/AGENT_PROJECT_BRIEF.md` (L01/L02/L03 course alignment guidance)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
